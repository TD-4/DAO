# API

## 1. datasets & dataloaders

### ğŸ³ClsDataset âˆš

[source](../core/modules/dataloaders/datasets/ClsDataset.py)

**æ„é€ å‡½æ•°**

```
class ClsDataset(Dataset):
    def __init__(self,
                 data_dir=None,
                 image_set="",
                 in_channels=1,
                 input_size=(224, 224),
                 preproc=None,
                 cache=False,
                 separator=":",
                 images_suffix=None):
        """
        åˆ†ç±»æ•°æ®é›†

        data_dir:str  æ•°æ®é›†æ–‡ä»¶å¤¹è·¯å¾„ï¼Œæ–‡ä»¶å¤¹è¦æ±‚æ˜¯
            |-dataset
                |- ç±»åˆ«1
                    |-å›¾ç‰‡
                |- ç±»åˆ«2
                |- ......
                |- train.txt
                |- val.txt
                |- test.txt
                |- labels.txt

        image_set:str "train.txt", "val.txt" or "test.txt"
        in_channels:int  è¾“å…¥å›¾ç‰‡çš„é€šé“æ•°ï¼Œç›®å‰åªæ”¯æŒ1å’Œ3é€šé“
        input_size:tuple è¾“å…¥å›¾ç‰‡çš„HW
        preproc:albumentations.Compose å¯¹å›¾ç‰‡è¿›è¡Œé¢„å¤„ç†
        cache:bool æ˜¯å¦å¯¹å›¾ç‰‡è¿›è¡Œå†…å­˜ç¼“å­˜
        separator:str labels.txt, train.txt, val.txt, test.txt çš„åˆ†å‰²ç¬¦ï¼ˆnameä¸idï¼‰
        images_suffix:list[str] å¯æ¥å—çš„å›¾ç‰‡åç¼€
        """
```

**configs**

```
"dataset": {
        "type": "ClsDataset",
        "kwargs": {
            "data_dir": "/root/data/DAO/screen",
            "image_set": "train.txt",
            "in_channels": 1,
            "input_size": [224, 224],
            "cache": True,
            "images_suffix": [".bmp"]
        },
        "transforms": {
            "kwargs": {
                "histogram": {"p": 1},
                "Normalize": {"mean": 0, "std": 1, "p": 1}
            }
        }
    }
```

### ğŸ›ClsDataloaderTrain âˆš

[source](../core/modules/dataloaders/ClsDataloader.py)

**æ„é€ å‡½æ•°**

```
def ClsDataloaderTrain(
        is_distributed=False,
        batch_size=None,
        num_workers=None,
        dataset=None,
        seed=0,
        **kwargs):
    """
    ClsDatasetçš„dataloaderç±»

    is_distributed:bool æ˜¯å¦æ˜¯åˆ†å¸ƒå¼
    batch_size: int batchsizeå¤§å°ï¼Œå¤šä¸ªGPUçš„batchsizeæ€»å’Œ
    num_workers:int ä½¿ç”¨çº¿ç¨‹æ•°
    dataset:ClsDatasetç±» æ•°æ®é›†ç±»çš„å®ä¾‹
    """
    
    ......
    return train_loader, max_iter
```

**configs**

```
 "dataloader": {
        "type": "ClsDataloaderTrain",
        "dataset": {
            "type": "ClsDataset",
            "kwargs": {
                "data_dir": "/root/data/DAO/screen",
                "image_set": "train.txt",
                "in_channels": 1,
                "input_size": [224, 224],
                "cache": True,
                "images_suffix": [".bmp"]
            },
            "transforms": {
                "kwargs": {
                    "histogram": {"p": 1},
                    "Normalize": {"mean": 0, "std": 1, "p": 1}
                }
            }
        },
        "kwargs": {
            "num_workers": 4,
            "batch_size": 256
        }
    },
```

### ğŸ¥©ClsDataloaderEval âˆš

[source](../core/modules/dataloaders/ClsDataloader.py)

**æ„é€ å‡½æ•°**

```
def ClsDataloaderEval(
        is_distributed=False,
        batch_size=None,
        num_workers=None,
        dataset=None,
        **kwargs):
    """
    ClsDatasetçš„dataloaderç±»

    is_distributed:bool æ˜¯å¦æ˜¯åˆ†å¸ƒå¼
    batch_size: int batchsizeå¤§å°ï¼Œå¤šä¸ªGPUçš„batchsizeæ€»å’Œ
    num_workers:int ä½¿ç”¨çº¿ç¨‹æ•°
    dataset:ClsDatasetç±» æ•°æ®é›†ç±»çš„å®ä¾‹
    """
    ......
    return val_loader, len(val_loader)
```

**configs**

```
"dataloader": {
    "type": "ClsDataloaderEval",
    "dataset": {
        "type": "ClsDataset",
        "kwargs": {
            "data_dir": "/root/data/DAO/screen",
            "image_set": "train.txt",
            "in_channels": 1,
            "input_size": [224, 224],
            "cache": True,
            "images_suffix": [".bmp"]
        },
        "transforms": {
            "kwargs": {
                "histogram": {"p": 1},
                "Normalize": {"mean": 0, "std": 1, "p": 1}
            }
        }
    },
    "kwargs": {
        "num_workers": 4,
        "batch_size": 256
    }
},
```

### 



### ğŸŸSegDataset

[source](../core/modules/dataloaders/datasets/SegDataset.py)

**æ„é€ å‡½æ•°**

```
Class SegDataset(data_dir=None, preproc=None, image_set="", in_channels=1, input_size=(224, 224), cache=False, image_suffix=".jpg", mask_suffix=".png"):
"""
	åˆ†å‰²æ•°æ®é›†

	data_dir:str  æ•°æ®é›†æ–‡ä»¶å¤¹è·¯å¾„ï¼Œæ–‡ä»¶å¤¹è¦æ±‚æ˜¯
           |-dataset
                |- images
                    |-å›¾ç‰‡
                |- masks

    image_set:str "train.txt or val.txt or test.txt"
    in_channels:int  è¾“å…¥å›¾ç‰‡çš„é€šé“æ•°ï¼Œç›®å‰åªæ”¯æŒ1å’Œ3é€šé“
    input_size:tuple è¾“å…¥å›¾ç‰‡çš„HW
    preproc:albumentations.Compose å¯¹å›¾ç‰‡è¿›è¡Œé¢„å¤„ç†
    cache:bool æ˜¯å¦å¯¹å›¾ç‰‡è¿›è¡Œå†…å­˜ç¼“å­˜
    images_suffix:str å¯æ¥å—çš„å›¾ç‰‡åç¼€
    mask_suffix:str å¯æ¥å—çš„å›¾ç‰‡åç¼€
"""
```

**config.json**

```
"dataset": {
	"type": "SegDataset",
	"kwargs": {
		"data_dir": "/root/data/DAO/VOC2012_Seg_Aug",
		"image_set": "val.txt",
        "in_channels": 3,
        "input_size": [380, 380],
        "cache": false,
        "image_suffix":".jpg",
        "mask_suffix":".png"
	},
    "transforms": {
    	"kwargs": {
    		"Resize": {"height": 224, "width": 224, "p": 1},
    		"Normalize": {"mean": [0.398993, 0.431193, 0.452234], "std": [0.285205, 0.273126, 0.276610], "p": 1}
		}
	}
}
```

### ğŸ¥—SegDataloaderTrain

[source](../core/modules/dataloaders/SegDataloader.py)

**æ„é€ å‡½æ•°**

```
def SegDataloaderTrain(is_distributed=False, batch_size=None, num_workers=None, dataset=None, seed=0)
"""
is_distributed : bool æ˜¯å¦æ˜¯åˆ†å¸ƒå¼
batch_size : int batchsizeå¤§å°
num_workers : int è¯»å–æ•°æ®çº¿ç¨‹æ•°
dataset : DotMap æ•°æ®é›†é…ç½®
seed : int éšæœºç§å­
"""
```

è¿”å›ç±»å‹

```
train_loader = DataPrefetcherSeg(train_loader)
return train_loader, max_iter
```

**configs.json**

```
    "dataloader": {
        "type": "SegDataloaderTrain",
        "dataset": {
	        "type": "SegDataset",
            "kwargs": {
                    "data_dir": "/root/data/DAO/VOC2012_Seg_Aug",
                    "image_set": "val.txt",
                    "in_channels": 3,
                    "input_size": [380, 380],
                    "cache": false,
                    "image_suffix":".jpg",
                    "mask_suffix":".png"
	            },
            "transforms": {
                "kwargs": {
                    "Resize": {"height": 224, "width": 224, "p": 1},
                    "Normalize": {"mean": [0.398993, 0.431193, 0.452234], "std": [0.285205, 0.273126, 0.276610], "p": 1}
                }
            }
        },
        "kwargs": {
            "num_workers": 4,
            "batch_size": 32
        }
    }
```

### ğŸŒ­SegDataloaderEval

[source](../core/modules/dataloaders/SegDataloader.py)

**æ„é€ å‡½æ•°**

```
def SegDataloaderEval(is_distributed=False, batch_size=None, num_workers=None, dataset=None):
    """
    is_distributed : bool æ˜¯å¦æ˜¯åˆ†å¸ƒå¼
    batch_size : int batchsizeå¤§å°
    num_workers : int è¯»å–æ•°æ®çº¿ç¨‹æ•°
    dataset : DotMap æ•°æ®é›†é…ç½®
    """
```

è¿”å›ç±»å‹

```
val_loader = torch.utils.data.DataLoader(valdataset, **dataloader_kwargs)
return val_loader, len(val_loader)
```



**configs.json**

```
"dataloader": {
            "type": "SegDataloaderEval",
            "dataset": {
                "type": "SegDataset",
                "kwargs": {
                    "data_dir": "/root/data/DAO/VOC2012_Seg_Aug",
                    "image_set": "val.txt",
                    "in_channels": 3,
                    "input_size": [380, 380],
                    "cache": false,
                    "image_suffix":".jpg",
                    "mask_suffix":".png"
                },
                "transforms": {
                    "kwargs": {
                        "Resize": {"height": 224, "width": 224, "p": 1},
                        "Normalize": {"mean": [0.398993, 0.431193, 0.452234], "std": [0.285205, 0.273126, 0.276610], "p": 1}

                    }
                }
            },
            "kwargs": {
                "num_workers": 4,
                "batch_size": 32
            }
        }
```

### MVTecDataset

```
MVTecDataset(
    data_dir=None,
    preproc=None,
    image_set="",
    in_channels=1,
    input_size=(224, 224),
    cache=False,
    image_suffix=".png",
    mask_suffix=".png",
    **kwargs
)
```

å¼‚å¸¸æ£€æµ‹æ•°æ®é›†ï¼Œï¼ˆMVTecDatasetç±»å‹ï¼‰

**1. æ„é€ å‡½æ•°**

- data_dir:str  æ•°æ®é›†æ–‡ä»¶å¤¹è·¯å¾„ï¼Œæ–‡ä»¶å¤¹è¦æ±‚æ˜¯
      ğŸ“‚datasets
      â”— ğŸ“‚your_custom_dataset
      â”£ ğŸ“‚ ground_truth
      â”ƒ â”£ ğŸ“‚ defective_type_1
      â”ƒ â”— ğŸ“‚ defective_type_2
      â”£ ğŸ“‚ test
      â”ƒ â”£ ğŸ“‚ defective_type_1
      â”ƒ â”£ ğŸ“‚ defective_type_2
      â”ƒ â”— ğŸ“‚ good
      â”— ğŸ“‚ train
      â”ƒ â”— ğŸ“‚ good
- preproc:albumentations.Compose å¯¹å›¾ç‰‡è¿›è¡Œé¢„å¤„ç†
- image_set:str "train.txt or val.txt or test.txt"
- in_channels:int  è¾“å…¥å›¾ç‰‡çš„é€šé“æ•°ï¼Œç›®å‰åªæ”¯æŒ1å’Œ3é€šé“
- input_size:tuple è¾“å…¥å›¾ç‰‡çš„HW
- cache:bool æ˜¯å¦å¯¹å›¾ç‰‡è¿›è¡Œå†…å­˜ç¼“å­˜
- image_suffix:str å¯æ¥å—çš„å›¾ç‰‡åç¼€
- mask_suffix:str å¯æ¥å—çš„å›¾ç‰‡åç¼€

## 2. models

### ğŸ•Backbones âˆš

[Pytorchè§†è§‰æ¨¡å‹åº“--timm](./models/timm_introduce.md) | [source](../core/modules/models/backbone/TIMM.py)

**æ„é€ å‡½æ•°**

```
def TIMM(backbone):
    """
    è·å–TIMMä¸»å¹²ç½‘ç»œ

    backbone:dict backbone:{kwargs:{è¿™é‡Œé¢æ˜¯timmåº“åˆ›å»ºmodelçš„å‚æ•°}}
    """
    # åˆ¤æ–­modelæ˜¯å¦åœ¨timmæ”¯æŒåˆ—è¡¨ä¸­
    if backbone.kwargs.model_name not in timm.list_models():
        logger.error("timm {} not supported {}".format(
            timm.__version__,
            backbone.kwargs.model_name))
        raise

    # åˆ¤æ–­modelæ˜¯å¦æœ‰pretrained
    if backbone.kwargs.pretrained and backbone.kwargs.model_name not in timm.list_models(pretrained=True):
        logger.error("{} hasn't pretrained weight, please set pretrained False".format(
            backbone.kwargs.model_name
        ))
        raise

    model = timm.create_model(**backbone.kwargs)
    return model

```



**configs.json**

```
"backbone": {
            "kwargs": {
                "model_name":"efficientnet_b0",
                "pretrained": true,
                "checkpoint_path": "",
                "exportable": true,
                "in_chans": 1,
                "num_classes": 38
            }
        }
```

éœ€è¦åµŒå…¥å…¶ä»–ç½‘ç»œä¸­ä½¿ç”¨

### ğŸ¿Classifications âˆš

[source](../core/modules/models/cls/TIMMC.py)

**æ„é€ å‡½æ•°**

```
def TIMMC(backbone_kwargs):
    backbone = Registers.backbones.get("TIMM")(backbone_kwargs)
    return backbone
```

**configs.json**

```
"model": {
        "type": "TIMMC",
        "summary_size": [1,224,224],
        "backbone": {
            "kwargs": {
                "model_name":"efficientnet_b0",
                "pretrained": true,
                "checkpoint_path": "",
                "exportable": true,
                "in_chans": 1,
                "num_classes": 38
            }
        },
        "kwargs": {
        }
    }
```

### ğŸ§‚Unet

[source](../core/modules/models/seg/unet/model.py)

**æ„é€ å‡½æ•°**

```
class Unet( encoder,
            encoder_depth=5,
            encoder_channels=None,
            decoder_use_batchnorm: bool = True,
            decoder_channels: List[int] = (256, 128, 64, 32, 16),
            decoder_attention_type: Optional[str] = None,
            num_classes=2,
            activation: Optional[Union[str, callable]] = None,
            aux_params: Optional[dict] = None,
    )
"""
encoder: dict encoderçš„é…ç½®å­—å…¸
encoder_depth: encoderæ·±åº¦
encoder_channels: encoder çš„æ¯ä¸€å±‚channelæ•°
decoder_use_batchnorm: bool
decoder_channels: List[int] = (256, 128, 64, 32, 16),
decoder_attention_type: Optional[str] = None,
num_classes=2,
activation: Optional[Union[str, callable]] = None,
aux_params: Optional[dict] = None,
"""
```

**configs.json**

```
"model": {
        "type": "Unet",
        "summary_size": [3,224,224],
        "backbone": {
            "kwargs": {
                "model_name": "resnet18",
                "pretrained": true,
                "checkpoint_path": "",
                "exportable": true,
                "in_chans": 3,
                "features_only": true
            }
        },
        "kwargs": {
            "encoder_depth": 5,
            "encoder_channels": [3, 64, 64, 128, 256, 512],
            "decoder_channels": [256, 128, 64, 32, 16],
            "num_classes": 21
        }
    }
```

### ğŸ¦UNet++

[source](../core/modules/models/seg/unetplusplus/model.py) | [note](https://github.com/FelixFu520/README/blob/main/train/segmentation/unetpp.md)

**æ„é€ å‡½æ•°**

```
class UnetPlusPlus(SegmentationModel):
    def __init__(
        self,
        encoder,
        encoder_depth=5,
        encoder_channels=None,
        decoder_use_batchnorm: bool = True,
        decoder_channels: List[int] = (256, 128, 64, 32, 16),
        decoder_attention_type: Optional[str] = None,
        num_classes=2,
        activation: Optional[Union[str, callable]] = None,
        aux_params: Optional[dict] = None,
    ):
    
    encoderï¼šCNNç½‘ç»œï¼Œ å¯¹åº”configä¸­çš„backbone
    encoder_depth: CNNçš„æ·±åº¦ï¼Œå³encoder_channelsçš„é•¿åº¦
    encoder_channels: CNNä¸»å¹²ç½‘ç»œæå–ç‰¹å¾çš„é€šé“æ•°
    decoder_use_batchnorm: æ„å»ºdecoderç½‘ç»œæ—¶æ˜¯å¦ä½¿ç”¨BN
    decoder_channelsï¼šdecoderæ—¶ï¼Œè¾“å‡ºçš„é€šé“æ•°
    num_classes: ç±»åˆ«æ•°ï¼Œ VOCï¼ˆ20fg+1bg)æ‰€ä»¥VOCæ•°æ®é›†æ—¶num_classesè®¾ä¸º21
    activation: æ„å»ºdecoderæ—¶ï¼Œæ˜¯å¦ä½¿ç”¨ç‰¹å®šçš„æ¿€æ´»å‡½æ•°
    aux_params: æ„å»ºUNethical++çš„é¢å¤–å‚æ•°
    
```

UnetPlusPlusé€šè¿‡`__init__`å‡½æ•°åˆå§‹åŒ–ï¼Œé€šè¿‡`forward`å‡½æ•°è¿”å›æƒ³è¦çš„å€¼ã€‚

**config**

```
 "model": {
        "type": "UnetPlusPlus",
        "summary_size": [3,224,224],
        "backbone": {
            "kwargs": {
                "model_name": "tf_mobilenetv3_small_075",
                "pretrained": true,
                "checkpoint_path": "",
                "exportable": true,
                "in_chans": 3,
                "features_only": true
            }
        },
        "kwargs": {
            "encoder_depth": 5,
            "encoder_channels": [3, 16, 16, 24, 40, 432],
            "decoder_channels": [256, 128, 64, 32, 16],
            "num_classes": 21
        }
    }
```

![](models/imgs/20220111150339.jpg)

### ğŸºPSPNet

[source](../core/modules/models/seg/pspnet/model.py) | [note](https://github.com/FelixFu520/README/blob/main/train/segmentation/pspnet.md)

**æ„é€ å‡½æ•°**

```
class PSPNet(SegmentationModel):
    """PSPNet_ is a fully convolution neural network for image semantic segmentation. Consist of
    *encoder* and *Spatial Pyramid* (decoder). Spatial Pyramid build on top of encoder and does not
    use "fine-features" (features of high spatial resolution). PSPNet can be used for multiclass segmentation
    of high resolution images, however it is not good for detecting small objects and producing accurate, pixel-level mask.
    Args:
        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)
            to extract features of different spatial resolution
        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features
            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features
            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).
            Default is 5
        encoder_weights: One of **None** (random initialization), **"imagenet"** (pre-training on ImageNet) and
            other pretrained weights (see table with available weights for each encoder_name)
        psp_out_channels: A number of filters in Spatial Pyramid
        psp_use_batchnorm: If **True**, BatchNorm2d layer between Conv2D and Activation layers
            is used. If **"inplace"** InplaceABN will be used, allows to decrease memory consumption.
            Available options are **True, False, "inplace"**
        psp_dropout: Spatial dropout rate in [0, 1) used in Spatial Pyramid
        in_channels: A number of input channels for the model, default is 3 (RGB images)
        classes: A number of classes for output mask (or you can think as a number of channels of output mask)
        activation: An activation function to apply after the final convolution layer.
            Available options are **"sigmoid"**, **"softmax"**, **"logsoftmax"**, **"tanh"**, **"identity"**, **callable** and **None**.
            Default is **None**
        upsampling: Final upsampling factor. Default is 8 to preserve input-output spatial shape identity
        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build
            on top of encoder if **aux_params** is not **None** (default). Supported params:
                - classes (int): A number of classes
                - pooling (str): One of "max", "avg". Default is "avg"
                - dropout (float): Dropout factor in [0, 1)
                - activation (str): An activation function to apply "sigmoid"/"softmax" (could be **None** to return logits)
    Returns:
        ``torch.nn.Module``: **PSPNet**
    .. _PSPNet:
        https://arxiv.org/abs/1612.01105
    """

    def __init__(
        self,
        encoder,
        encoder_channels=None,
        psp_out_channels: int = 512,
        psp_use_batchnorm: bool = True,
        psp_dropout: float = 0.2,
        num_classes: int = 1,
        activation: Optional[Union[str, callable]] = None,
        upsampling: int = 8,
        aux_params: Optional[dict] = None,
    ):
```

**configs**

```
"model": {
    "type": "PSPNet",
    "summary_size": [3,224,224],
    "backbone": {
        "kwargs": {
            "model_name": "resnet50",
            "pretrained": true,
            "checkpoint_path": "",
            "exportable": true,
            "in_chans": 3,
            "features_only": true
        }
    },
    "kwargs": {
        "encoder_channels": [3, 64, 256, 512, 1024, 2048],
        "psp_out_channels": 512,
        "num_classes": 21,
        "upsampling": 32
    }
},
```

![](models/imgs/20220113145704.jpg)

### Deeplab

[source](../core/modules/models/seg/deeplab/model.py) | [note](https://github.com/FelixFu520/README/blob/main/train/segmentation/pspnet.md)

### Yolox

[source](../core/modules/models/seg/deeplab/model.py) | [note](https://github.com/FelixFu520/README/blob/main/train/segmentation/pspnet.md)

## 3. optims

### ğŸ”sgd_warmup_bias_bn_weight âˆš

[source](../core/modules/optims/sgd_warmup_bias_bn_weight.py)

**æ„é€ å‡½æ•°**

```
def sgd_warmup_bias_bn_weight(model=None,
                              lr=0.01,
                              weight_decay=1e-4,
                              momentum=0.9,
                              warmup_lr=0,
                              warmup_epoch=5
                              ):
    """
    model:torch.nn.Module æ­¤trainerçš„self.modelå±æ€§
    lr: float å¯¹äºæ•´ä¸ªï¼ˆå¤šæœºå¤šå¡ï¼‰batch sizeçš„å­¦ä¹ ç‡
    weight_decay:float torch.optim.SGD é»˜è®¤å‚æ•°
    momentum:float torch.optim.SGD é»˜è®¤å‚æ•°
    warmup_lr:float warmupæ—¶çš„å­¦ä¹ ç‡
    warmup_epoch:int warmupå‡ ä¸ªepoch
    """
```

**configs.json**

```
   "optimizer": {
        "type": "sgd_warmup_bias_bn_weight",
        "kwargs": {
            "lr": 0.01,
            "weight_decay": 1e-4,
            "momentum": 0.9,
            "warmup_lr": 0,
            "warmup_epoch": 5
        }
    }
```



- 

**2.configs.json**

## 4. losses

### ğŸ—CrossEntropyLoss âˆš

[source](../core/modules/losses/CrossEntropyLoss.py)

**æ„é€ å‡½æ•°**

```
def CrossEntropyLoss(weight=None, ignore_index=255, reduction='mean')

```

**configs.json**

```
"loss": {
        "type": "CrossEntropyLoss",
        "kwargs": {
            "ignore_index": 255,
            "weight": [
                1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
                1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
                1
            ],
            "reduction": "mean"
        }
    }
```

## 5. scheduler

### ğŸ–warm_cos_lr âˆš

[source](../core/modules/schedulers/warm_cos_lr.py)

**æ„é€ å‡½æ•°**

```
class warm_cos_lr(self, lr, iters_per_epoch, total_epochs, **kwargs):
        """
        Args:
            lr (float): learning rate.
            iters_per_peoch (int): number of iterations in one epoch.
            total_epochs (int): number of epochs in training.
            kwargs (dict):
                - warmup_epochs
                - warmup_lr_start (default 1e-6)
        """
```

**è°ƒç”¨æ–¹æ³•**

```
def update_lr(self, iters) æ›´æ–°lr
```

**configs**

```
"lr_scheduler": {
        "type": "warm_cos_lr",
        "kwargs": {
            "warmup_epochs": 5,
            "warmup_lr_start": 0
        }
    }
```

## 6. evaluators

### ğŸ˜SegEvaluator

[source](../core/modules/evaluators/SegEvaluator.py)

**æ„é€ å‡½æ•°**

```
class SegEvaluator(is_distributed=False, dataloader=None, num_classes=None)
```

**è°ƒç”¨æ–¹æ³•**

```
evaluate(self, model, distributed=False, half=False, device=None)
```

**configs**

```
 "evaluator": {
        "type": "SegEvaluator",
        "dataloader": {
            "type": "SegDataloaderEval",
            "dataset": {
                "type": "SegDataset",
                "kwargs": {
                    "data_dir": "/root/data/DAO/VOC2012_Seg_Aug",
                    "image_set": "val.txt",
                    "in_channels": 3,
                    "input_size": [380, 380],
                    "cache": false,
                    "image_suffix":".jpg",
                    "mask_suffix":".png"
                },
                "transforms": {
                    "kwargs": {
                        "Resize": {"height": 224, "width": 224, "p": 1},
                        "Normalize": {"mean": [0.398993, 0.431193, 0.452234], "std": [0.285205, 0.273126, 0.276610], "p": 1}
                    }
                }
            },
            "kwargs": {
                "num_workers": 4,
                "batch_size": 32
            }
        }

    }
```

### ğŸ ClsEvaluator âˆš

[source](../core/modules/evaluators/ClsEvaluator.py)

**æ„é€ å‡½æ•°**

```
class ClsEvaluator:
    def __init__(self,
                 is_distributed=False,
                 dataloader=None,
                 num_classes=None,
                 is_industry=False,
                 industry=None,
                 target_layer="conv_head"):
        """
        éªŒè¯å™¨
        is_distributed:bool æ˜¯å¦æ˜¯åˆ†å¸ƒå¼
        dataloader:dict dataloaderçš„é…ç½®å­—å…¸
        num_classes:int ç±»åˆ«æ•°
        is_industry:bool æ˜¯å¦ä½¿ç”¨å·¥ä¸šæ–¹æ³•éªŒè¯ï¼Œå³è¾“å‡ºè¿‡æ¼æ£€
        industry:dict ä½¿ç”¨å·¥ä¸šéªŒè¯æ–¹æ³•æ‰€éœ€çš„å‚æ•°
        """
```

**configs**

```
"evaluator": {
        "type": "ClsEvaluator",
        "dataloader": {
            "type": "ClsDataloaderEval",
            "dataset": {
                "type": "ClsDataset",
                "kwargs": {
                    "data_dir": "/root/data/DAO/screen",
                    "image_set": "val.txt",
                    "in_channels": 1,
                    "input_size": [224, 224],
                    "cache": false,
                    "images_suffix": [".bmp"]
                },
                "transforms": {
                    "kwargs": {
                        "histogram": {"p": 1},
                        "Normalize": {"mean": 0, "std": 1, "p": 1}
                    }
            }
            },
            "kwargs": {
                "num_workers": 4,
                "batch_size": 256
            }
        },
        "kwargs": {
            "num_classes": 38,
            "is_industry": false
        }

    }
```

## 7. trainer

### ğŸ¥¨ClsTrainer

